{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562aabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, roc_auc_score, \\\n",
    "    recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "import json\n",
    "import mlflow\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.stats.api as sms\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bf215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccf5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(123)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "python_random.seed(123)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e0ac790",
   "metadata": {},
   "source": [
    "search_space_lstm = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'lstm',\n",
    "        'activation': hp.choice('activation', [\"relu\", \"tanh\"]),\n",
    "        'units': hp.quniform('units', 32, 1024, 32),\n",
    "        'batch': hp.choice('batch', [2048]),\n",
    "        'epochs': hp.quniform('epochs', 10, 50, 10),\n",
    "        'dropout': hp.choice('dropout', [True, False]),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.01)),\n",
    "        'preprocessing': hp.choice('p_lstm', ['scaler', 'filter', 'all', 'none', 'fi_ss',\n",
    "                                    'fi_sm', 'ss_sm', 'smote'])\n",
    "    }\n",
    "])\n",
    "\n",
    "search_space_lstm = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'lstm',\n",
    "        'activation': hp.choice('activation', [\"relu\"]),\n",
    "        'units': hp.quniform('units', 480, 1024, 32),\n",
    "        'batch': hp.choice('batch', [256, 512]), # , 1024, 2048\n",
    "        'epochs': hp.quniform('epochs', 20, 30, 10),\n",
    "        'dropout': hp.choice('dropout', [True, False]),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.006)),\n",
    "        'preprocessing': hp.choice('p_lstm', ['scaler', 'filter', 'all', 'none', 'fi_ss',\n",
    "                                    'fi_sm', 'ss_sm', 'smote']), \n",
    "        'sequences': hp.choice('sequences', [5]) # , 5, 7\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b467d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_lstm = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'lstm',\n",
    "        'activation': hp.choice('activation', [\"relu\"]),\n",
    "        'units': hp.quniform('units', 512, 1024, 32),\n",
    "        'batch': hp.choice('batch', [256, 512]), # , 1024, 2048\n",
    "        'epochs': hp.choice('epochs', [20]),\n",
    "        'dropout': hp.choice('dropout', [True, False]),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.0006)),\n",
    "        'preprocessing': hp.choice('p_lstm',  ['scaler', 'filter', 'none', 'fi_ss']), \n",
    "        'sequences': hp.choice('sequences', [5]) # , 5, 7\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf99125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_0(y):\n",
    "    if y['INDISPONIBILIDADE'] == 0:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "def f_1(y):\n",
    "    if y['INDISPONIBILIDADE'] == 1:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "def ajusta_y(y):\n",
    "    y['0'] = y.apply(f_0, axis=1)\n",
    "    y['1'] = y.apply(f_1, axis=1)\n",
    "    y = y[['0', '1']]\n",
    "    return y\n",
    "\n",
    "\n",
    "#def create_sequences(values, time_steps=1):\n",
    "#    output = []\n",
    "#    for i in range(len(values) - time_steps + 1):\n",
    "#        output.append(values[i:(i + time_steps)])\n",
    "#    return np.stack(output)\n",
    "\n",
    "\n",
    "def create_sequences(values, time_steps=1):\n",
    "    return np.asarray([values[i : (i + time_steps)] for i in range(len(values) - time_steps + 1)])\n",
    "\n",
    "\n",
    "def ajusta_y_timestep(y, time_steps=1):\n",
    "    new_y = y[time_steps-1:]\n",
    "    return new_y\n",
    "\n",
    "\n",
    "def transform_dimension_timesteps(train_x, train_y, time_steps=1):\n",
    "\n",
    "    train_x = create_sequences(train_x, time_steps)\n",
    "    train_y = ajusta_y_timestep(train_y, time_steps)\n",
    "    train_y = train_y.values.reshape(-1, 2)\n",
    "    \n",
    "    print(train_y.shape)\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def ajusta_saida(y_pred):\n",
    "    y_pred_c = []\n",
    "    for x in y_pred:\n",
    "        y_pred_c.append(np.argmax(x))\n",
    "    return y_pred_c\n",
    "\n",
    "\"\"\"def objective_lstm(params):\n",
    "    units = params['units']\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(params['units'], activation=params['activation'],\n",
    "                   return_sequences=False, input_shape=(1, shape)))\n",
    "    \n",
    "    model.add(Dense(params['units'], activation=params['activation'],\n",
    "                    input_shape=(784,)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=params['epochs'],\n",
    "              batch_size=params['batch_size'], verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return -score[1]\n",
    "    \n",
    "# sequence 1 multilayer\n",
    "\n",
    "def build(self, **kwargs):\n",
    "        activation = kwargs.get('activation')\n",
    "        shape = kwargs.get('shape')\n",
    "        batch = int(kwargs.get('batch'))\n",
    "        dropout = kwargs.get('dropout')\n",
    "        lr = kwargs.get('learning_rate')\n",
    "        sequences = int(float(kwargs.get('learning_rate')))\n",
    "        units = int(float(kwargs.get('units')))\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=False,\n",
    "                       input_shape=(1, shape)))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        if dropout:\n",
    "            model.add(Dropout(rate=0.2))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=RMSprop(learning_rate=lr),\n",
    "                      metrics=[get_f1])\n",
    "        return model\n",
    "\n",
    "\n",
    " kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "class MyModel():\n",
    "    def build(self, **kwargs):\n",
    "        activation = kwargs.get('activation')\n",
    "        shape = kwargs.get('shape')\n",
    "        batch = int(kwargs.get('batch'))\n",
    "        dropout = kwargs.get('dropout')\n",
    "        lr = kwargs.get('learning_rate')\n",
    "        sequences = int(float(kwargs.get('sequences')))\n",
    "        units = int(float(kwargs.get('units')))\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=True,\n",
    "                       input_shape=(sequences, shape), kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=False,\n",
    "                       input_shape=(sequences, shape), kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "        if dropout:\n",
    "            model.add(Dropout(rate=0.2))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=RMSprop(learning_rate=lr),\n",
    "                      metrics=[get_f1])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def fit(self, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=batch,\n",
    "            **kwargs, verbose=0\n",
    "        )\n",
    "\n",
    "\n",
    "def predict_keras(model, test_x, s=1):\n",
    "    x = create_sequences(test_x.copy(), s)\n",
    "    predicted = model.predict(x)\n",
    "    # details(predicted)\n",
    "    predicted = ajusta_saida(predicted)\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def test_model(model, l, x_train, y_train, x_val, y_val):\n",
    "    batch, epochs, sequences = l\n",
    "    # print(\"Converting training data\")\n",
    "    # x, y = transform_dimension_timesteps(train_x.copy(), train_y)\n",
    "    print(\"Limpando a sessão\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"Excutando a limpeza de lixo\")\n",
    "    gc.collect()\n",
    "    print(\"Training the model\")\n",
    "    model.fit(x_train, y_train, batch_size=batch, epochs=epochs)\n",
    "\n",
    "    pred = predict_keras(model, x_val, sequences)\n",
    "    y_val = y_val[sequences-1:]\n",
    "    f1 = f1_score(y_val, pred)\n",
    "    print(confusion_matrix(y_val, pred))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def split(r, mat):\n",
    "    i = int(len(mat)*r)\n",
    "\n",
    "    return mat[:i], mat[i:]\n",
    "\n",
    "\n",
    "def train_test(train, test):\n",
    "\n",
    "    x_train = train.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_train = train[['INDISPONIBILIDADE']]\n",
    "\n",
    "    x_test = test.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_test = test[['INDISPONIBILIDADE']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def find_best_keras(df, evals):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2156b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    f1 = f1_score(actual, pred)\n",
    "    roc = roc_auc_score(actual, pred)\n",
    "    rec = recall_score(actual, pred)\n",
    "    pre = precision_score(actual, pred)\n",
    "    acc = accuracy_score(actual, pred)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    print(confusion_matrix(actual, pred))\n",
    "    return f1, roc, rec, pre, acc\n",
    "\n",
    "\n",
    "def model_selection(m, x, y, p, c, clf):\n",
    "    if m == 'train_test':\n",
    "        # Ratio train test split\n",
    "        r = 0.75\n",
    "        return train_test_selection(r, x, y, p, c, clf)\n",
    "\n",
    "\n",
    "\n",
    "def objective_keras(params):\n",
    "    \n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        x, y = data\n",
    "        p = params['preprocessing']\n",
    "        print(\"Preprocess:\", p)\n",
    "        df = pd.concat([x.reset_index(drop=True),\n",
    "                    y.reset_index(drop=True)], axis=1) \n",
    "        train, test = split(.75, df)\n",
    "        x_train, y_train, x_val, y_val = train_test(train, test)\n",
    "\n",
    "        x_train, x_val, y_train = preprocessing(p, x_train, x_val, y_train)\n",
    "        print(\"Converting training data\")\n",
    "        y_train = ajusta_y(y_train)\n",
    "        x_train, y_train = transform_dimension_timesteps(x_train, y_train, time_steps=params['sequences'])\n",
    "        \n",
    "        \n",
    "        del params['preprocessing']\n",
    "        del params['type']\n",
    "        mlflow.log_param(\"model\", 'lstm')\n",
    "        mlflow.log_param(\"model_selection\", split_strategy)\n",
    "        mlflow.log_param(\"stage\", 'tuning')\n",
    "\n",
    "        params['shape'] = x_train.shape[2]\n",
    "        clf = MyModel().build(**params)\n",
    "        l = [int(params['batch']), int(params['epochs']), int(params['sequences'])]\n",
    "\n",
    "        print(params)\n",
    "        \n",
    "        f1 = 0\n",
    "        n = 3\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            try:\n",
    "                f1 = f1 + test_model(clf, l, x_train, y_train, x_val, y_val)\n",
    "            except:\n",
    "                print(\"Houve um erro ao testar o modelo, tentando mais uma vez\")\n",
    "                time.sleep(2)\n",
    "                print(\"Limpando a sessão\")\n",
    "                tf.keras.backend.clear_session()\n",
    "                print(\"Excutando a limpeza de lixo\")\n",
    "                gc.collect()\n",
    "                f1 = f1 + test_model(clf, l, x_train, y_train, x_val, y_val)\n",
    "                print(\"F1:\", f1)\n",
    "        \n",
    "        f1 = f1/n\n",
    "\n",
    "        print(\"Média F1-SCORE\", f1)\n",
    "        mlflow.log_metric(\"f1_val\", f1)\n",
    "\n",
    "        # Because fmin() tries to minimize the objective,\n",
    "        # this function must return the negative accuracy.\n",
    "        return {'loss': -f1, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def get_best(key):\n",
    "    f = open('params/best_hyper.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data[key]\n",
    "\n",
    "\n",
    "def find_best(x, y, evals, space):\n",
    "    \n",
    "\n",
    "    global data\n",
    "    data = [x, y]\n",
    "    rstate = np.random.default_rng(42)\n",
    "    trials = Trials()\n",
    "    best_result = fmin(\n",
    "        fn=objective_keras, space=space,\n",
    "        algo=tpe.suggest, max_evals=evals,\n",
    "        trials=trials, rstate=rstate)\n",
    "\n",
    "    result = hyperopt.space_eval(space, best_result)\n",
    "    print(\"Best in Search Space:\", result)\n",
    "    print('trials:')\n",
    "    for trial in trials.trials[:2]:\n",
    "        print(trial)\n",
    "\n",
    "    key = result['type']\n",
    "    del result['type']\n",
    "    # update_hyper(result, key)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    return result, trials, key\n",
    "\n",
    "\n",
    "def split(r, mat):\n",
    "    i = int(len(mat)*r)\n",
    "\n",
    "    return mat[:i], mat[i:]\n",
    "\n",
    "\n",
    "def preprocess(filtering, scaler, smote, x_train, x_test, y_train):\n",
    "\n",
    "    if filtering == 'True':\n",
    "        print(\"Filtering\")\n",
    "        with open('../data/params/features.pkl', 'rb') as inp:\n",
    "            features = pickle.load(inp)\n",
    "        x_train = x_train[features]\n",
    "        x_test = x_test[features]\n",
    "\n",
    "    if scaler == 'True':\n",
    "        print(\"Standard Scale\")\n",
    "        ss = StandardScaler() # .set_output(transform=\"pandas\")\n",
    "        ss.fit(x_train)\n",
    "        x_train = ss.transform(x_train)\n",
    "        x_test = ss.transform(x_test)\n",
    "\n",
    "    if smote == 'True':\n",
    "        print(\"SMOTE\")\n",
    "\n",
    "        # if isinstance(x_train, cd.DataFrame):\n",
    "        #    x_train, y_train = x_train.to_pandas(), y_train.to_pandas()\n",
    "        \n",
    "        with open(\"../data/params/smote.pkl\", \"rb\") as inp:\n",
    "            samp_strat = pickle.load(inp)\n",
    "        print(\"Sampling Strategy: \", samp_strat)\n",
    "        smote = SMOTE(random_state=42, sampling_strategy=samp_strat)\n",
    "        x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "        # x_train, y_train = cd.from_pandas(x_train), cd.from_pandas(y_train)\n",
    "\n",
    "    return x_train, x_test, y_train\n",
    "\n",
    "\n",
    "def preprocessing(p, x_train, x_test,  y_train, ):\n",
    "\n",
    "    if p == 'all':\n",
    "        x_train, x_test, y_train = preprocess('True', 'True', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'filter':\n",
    "        x_train, x_test, y_train = preprocess('True', 'False', 'False',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'scaler':\n",
    "        x_train, x_test, y_train = preprocess('False', 'True', 'False',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'smote':\n",
    "        x_train, x_test, y_train = preprocess('False', 'False', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'fi_sm':\n",
    "        x_train, x_test, y_train = preprocess('True', 'False', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'fi_ss':\n",
    "        x_train, x_test, y_train = preprocess('True', 'True', 'False',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'ss_sm':\n",
    "        x_train, x_test, y_train = preprocess('False', 'True', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "\n",
    "    return x_train, x_test, y_train\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    mat = pd.read_csv('../data/raw/matomo.csv', dtype=np.int32)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def train_test(train, test):\n",
    "\n",
    "    x_train = train.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_train = train[['INDISPONIBILIDADE']]\n",
    "\n",
    "    x_test = test.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_test = test[['INDISPONIBILIDADE']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def get_model(k, params):\n",
    "    if k == 'knn':\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "    elif k == 'svm':\n",
    "        clf = SVC(**params)\n",
    "    elif k == 'nb':\n",
    "        clf = GaussianNB(**params)\n",
    "    elif k == 'rf':\n",
    "        clf = RandomForestClassifier(**params)\n",
    "    elif k == 'ada':\n",
    "        clf = AdaBoostClassifier(**params)\n",
    "    elif k == 'dt':\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "    elif k == 'lstm':\n",
    "        clf = MyModel().build(**params)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def test_one(key, params, x_train, y_train, x_test, y_test):\n",
    "    p = params['preprocessing']\n",
    "    del params['preprocessing']\n",
    "\n",
    "    x_train, x_test, y_train = preprocessing(p, x_train,\n",
    "                                             x_test, y_train)\n",
    "\n",
    "    model = get_model(key, params)\n",
    "\n",
    "    if key in ('ada', 'dt', 'nb'):\n",
    "        model.fit(x_train.to_pandas(), y_train.to_pandas())\n",
    "        pred = model.predict(x_test.to_pandas())\n",
    "        f1, roc, rec, pre, acc = eval_metrics(y_test.to_pandas(), pred)\n",
    "    else:\n",
    "        y_train = y_train['INDISPONIBILIDADE'].values\n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        f1, roc, rec, pre, acc = eval_metrics(y_test.to_pandas(),\n",
    "                                              pred.to_pandas())\n",
    "\n",
    "    return f1, roc, rec, pre, acc\n",
    "\n",
    "\n",
    "def test_params(x_train, y_train, x_test, y_test, params):\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "    for key in params:\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_param(\"model\", key)\n",
    "            mlflow.log_param(\"model_selection\", split_strategy)\n",
    "            mlflow.log_param(\"stage\", \"Testing_algos\")\n",
    "            mlflow.log_params(params[key])\n",
    "\n",
    "            print(key)\n",
    "            f1, roc, rec, pre, acc = test_one(key, params[key],\n",
    "                                              x_train, y_train,\n",
    "                                              x_test, y_test)\n",
    "\n",
    "            mlflow.log_metric('f1', f1)\n",
    "            mlflow.log_metric('roc', roc)\n",
    "            mlflow.log_metric('recall', rec)\n",
    "            mlflow.log_metric('precision', pre)\n",
    "            mlflow.log_metric('accuracy', acc)\n",
    "\n",
    "\n",
    "def delete_runs():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    runs = mlflow.search_runs()\n",
    "\n",
    "    for run in runs.iterrows():\n",
    "        mlflow.delete_run(run[1].run_id)\n",
    "\n",
    "\n",
    "def is_sklearn(model):\n",
    "    if type(model).__module__[:7] == 'sklearn':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def ajust_columns(results):\n",
    "    for c in results.columns:\n",
    "        if c[:7] == \"params.\":\n",
    "            results = results.rename(columns={c: c[7:]})\n",
    "\n",
    "    results = results.rename(columns={\"metrics.f1_val\": \"f1_val\"})\n",
    "    return results\n",
    "\n",
    "\n",
    "def correct_parameters(best_results):\n",
    "    for result in best_results:\n",
    "        if result != \"knn\":\n",
    "            try:\n",
    "                del best_results[result][\"n_neighbors\"]\n",
    "                del best_results[result][\"metric\"]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    for k in best_results:\n",
    "        for p in best_results[k]:\n",
    "            if p in (\"n_estimators\", \"n_neighbors\"):\n",
    "                best_results[k][p] = int(best_results[k][p])\n",
    "            if p in (\"C\", \"var_smoothing\", \"learning_rate\"):\n",
    "                best_results[k][p] = float(best_results[k][p])\n",
    "\n",
    "    return best_results\n",
    "\n",
    "\n",
    "def get_best_parameters(split_strategy):\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    results = mlflow.search_runs()\n",
    "\n",
    "    results = ajust_columns(results)\n",
    "    query = f'model_selection == \"{split_strategy}\"'\n",
    "    grouped = results.query(query).groupby(\"type\")\n",
    "    indices_max = grouped[\"f1_val\"].idxmax()\n",
    "    best_results = {}\n",
    "\n",
    "    for modelo, indice in indices_max.items():\n",
    "        parametros = results.loc[\n",
    "            indice,\n",
    "            [\n",
    "                \"preprocessing\",\n",
    "                \"C\",\n",
    "                \"kernel\",\n",
    "                \"n_estimators\",\n",
    "                \"n_neighbors\",\n",
    "                \"criterion\",\n",
    "                \"var_smoothing\",\n",
    "                \"learning_rate\",\n",
    "                \"metric\",\n",
    "                \"units\",\n",
    "                \"activation\",\n",
    "                \"batch\",\n",
    "                \"dropout\", \n",
    "                \"epochs\"\n",
    "            ],\n",
    "        ]\n",
    "        parametros = {\n",
    "            chave: valor\n",
    "            for chave, valor in parametros.to_dict().items()\n",
    "            if type(valor) == str\n",
    "        }\n",
    "        best_results[modelo] = parametros\n",
    "\n",
    "    return correct_parameters(best_results)\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "    print(\"Reading data\")\n",
    "    mat = get_data()\n",
    "    print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "    train, test = split(0.75, mat)\n",
    "    print(\"Spliting the data into x and y features\")\n",
    "    x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "\n",
    "   \n",
    "    print(\"Find best parameters for LSTM model\")\n",
    "    find_best(x_train, y_train, 50, search_space_lstm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c0c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(var):\n",
    "    print(\"Detalhes da variavel:\")\n",
    "    print(var)\n",
    "    #print(\"shape\", var.shape)\n",
    "    print(\"Type\", type(var))\n",
    "    print(\"Soma\", sum(var))\n",
    "\n",
    "def eval_one_variance(test, model, key):\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    results = []\n",
    "    x_test = test.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_test = test[[\"INDISPONIBILIDADE\"]]\n",
    "    add = int(len(x_test)*.04)\n",
    "    y = -add\n",
    "    l = int(len(x_test)*.6)\n",
    "    for i in range(10):\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_param(\"model\", key)\n",
    "            mlflow.log_param(\"stage\", \"statistics_analysis\")\n",
    "            mlflow.log_param(\"random_i\", i)\n",
    "            mlflow.tensorflow.autolog()\n",
    "\n",
    "            # test_shuffle = test.sample(frac=0.5, random_state=i)\n",
    "\n",
    "            # x_test = test_shuffle.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "            # y_test = test_shuffle[[\"INDISPONIBILIDADE\"]]\n",
    "\n",
    "            # create a for that get 50% of data plus 5% at each iteration\n",
    "\n",
    "            y = y + add\n",
    "            n = l + y\n",
    "            x_test_copy = x_test[:n].copy()\n",
    "            y_test_copy = y_test[:n].copy()\n",
    "\n",
    "            pred = predict_keras(model, x_test_copy, 5)\n",
    "            \n",
    "            y_test_copy = y_test_copy[4:].values\n",
    "            # details(pred)\n",
    "            f1, roc, rec, pre, acc = eval_metrics(y_test_copy, pred)\n",
    "            results.append(f1)\n",
    "\n",
    "            mlflow.log_metric(\"f1\", f1)\n",
    "            mlflow.log_metric(\"roc\", roc)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"precision\", pre)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    media = np.mean(results)\n",
    "    dp = np.std(results, ddof=1)\n",
    "    ci = sms.DescrStatsW(results).tconfint_mean()\n",
    "    return media, dp, ci\n",
    "\n",
    "\n",
    "def eval_variance(x_train, y_train, x_test, y_test, params):\n",
    "    metricas = {}\n",
    "    for k in params:\n",
    "        if k in [\"lstm\"]:\n",
    "            print(\"Algo:\", k)\n",
    "            p = params[k][\"preprocessing\"]\n",
    "            del params[k][\"preprocessing\"]\n",
    "            (\n",
    "                x_train_c,\n",
    "                x_test_c,\n",
    "                y_train_c,\n",
    "            ) = preprocessing(p, x_train, x_test, y_train)\n",
    "            params[k][\"shape\"] = x_train_c.shape[1]\n",
    "            print(params[k])\n",
    "            model = MyModel().build(**params[k])\n",
    "\n",
    "            print(\"Ajusting y\")\n",
    "            y_train_c = ajusta_y(y_train_c)\n",
    "            print(\"Transforming dimension\")\n",
    "            x_train_c, y_train_c = transform_dimension_timesteps(x_train_c, y_train_c, time_steps=5)\n",
    "            # y_train_c = y_train_c[\"INDISPONIBILIDADE\"].values\n",
    "            batch = int(params[k][\"batch\"])\n",
    "            # Imprindo o shape dos dados\n",
    "            # print(\"x_train_c, y_train_c\", x_train_c.shape, y_train_c.shape)\n",
    "            model.fit(x_train_c, y_train_c, batch_size=batch, epochs=int(float(params[k][\"epochs\"])))\n",
    "            \"\"\"\n",
    "            (\n",
    "                x_train_c,\n",
    "                x_test_c,\n",
    "                y_train_c,\n",
    "            ) = preprocessing(p, x_train, x_test, y_train)\n",
    "            \"\"\"\n",
    "            # Salvar o modelo no MLflow\n",
    "            #mlflow.tensorflow.save_model(tf_saved_model=model, path=\"../models/lstm_final\")\n",
    "\n",
    "            x_test_c = pd.DataFrame(x_test_c)\n",
    "            test = pd.concat(\n",
    "                [x_test_c.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1\n",
    "            )\n",
    "            try:\n",
    "                media, dp, ci = eval_one_variance(test, model, k)\n",
    "            except Exception as e:\n",
    "                # Tratamento da exceção e apresentação do erro\n",
    "                print(\"Falha ao calcular o intervalo de confiança\")\n",
    "                print(f\"Ocorreu uma exceção: {e}\")\n",
    "                return model, test, model, k\n",
    "            metricas[k] = {}\n",
    "            metricas[k][\"mean\"] = media\n",
    "            metricas[k][\"stand_dev\"] = dp\n",
    "            metricas[k][\"conf_int\"] = ci\n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global split_strategy\n",
    "split_strategy = 'train_test'\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016812d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Limit GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0f6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Spliting the data into train/test with 75/25 proportion\n",
      "Spliting the data into x and y features\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "mat = get_data()\n",
    "print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "train, test = split(0.75, mat)\n",
    "print(\"Spliting the data into x and y features\")\n",
    "x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "split_strategy = \"train_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84a6e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: lstm\n",
      "Filtering\n",
      "{'learning_rate': 5e-05, 'units': '800.0', 'activation': 'tanh', 'batch': '256', 'dropout': 'True', 'epochs': '20.0', 'shape': 87, 'sequences': 5}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5, 800)            2841600   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 800)               5123200   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 800)               640800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1602      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,607,202\n",
      "Trainable params: 8,607,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Ajusting y\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### params = get_best_parameters(split_strategy)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.00005\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m800.0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m256\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20.0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m87\u001b[39m, \n\u001b[0;32m      4\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m}}\n\u001b[1;32m----> 5\u001b[0m metricas \u001b[38;5;241m=\u001b[39m \u001b[43meval_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36meval_variance\u001b[1;34m(x_train, y_train, x_test, y_test, params)\u001b[0m\n\u001b[0;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel()\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[k])\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAjusting y\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m y_train_c \u001b[38;5;241m=\u001b[39m \u001b[43majusta_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransforming dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m x_train_c, y_train_c \u001b[38;5;241m=\u001b[39m transform_dimension_timesteps(x_train_c, y_train_c, time_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36majusta_y\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21majusta_y\u001b[39m(y):\n\u001b[1;32m---> 18\u001b[0m     y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mapply(f_0, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m     y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mapply(f_1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m     y \u001b[38;5;241m=\u001b[39m y[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py:3845\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3842\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[0;32m   3843\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py:3810\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 3810\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_setitem_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\generic.py:4030\u001b[0m, in \u001b[0;36mNDFrame._check_setitem_copy\u001b[1;34m(self, t, force)\u001b[0m\n\u001b[0;32m   4028\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m com\u001b[38;5;241m.\u001b[39mSettingWithCopyError(t)\n\u001b[0;32m   4029\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4030\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(t, com\u001b[38;5;241m.\u001b[39mSettingWithCopyWarning, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[43mfind_stack_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\util\\_exceptions.py:32\u001b[0m, in \u001b[0;36mfind_stack_level\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_stack_level\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Find the first place in the stack that is not inside pandas\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    (tests notwithstanding).\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     pkg_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pd\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\inspect.py:1554\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\inspect.py:1531\u001b[0m, in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1529\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[1;32m-> 1531\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1532\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo))\n\u001b[0;32m   1533\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\inspect.py:1505\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1503\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1505\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1507\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\inspect.py:829\u001b[0m, in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m    827\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 829\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[0;32m    831\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\inspect.py:755\u001b[0m, in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    752\u001b[0m         f \u001b[38;5;241m=\u001b[39m getabsfile(module)\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[0;32m    754\u001b[0m         modulesbyfile[f] \u001b[38;5;241m=\u001b[39m modulesbyfile[\n\u001b[1;32m--> 755\u001b[0m             \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(modulesbyfile[file])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\ntpath.py:664\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# Ensure that the non-prefixed path resolves to the same path\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_getfinalpathname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m path:\n\u001b[0;32m    665\u001b[0m         path \u001b[38;5;241m=\u001b[39m spath\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;66;03m# If the path does not exist and originally did not exist, then\u001b[39;00m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;66;03m# strip the prefix anyway.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### params = get_best_parameters(split_strategy)\n",
    "params = {'lstm': {'learning_rate': 0.00005, 'units': '800.0', 'activation': 'tanh', \n",
    "                   'batch': '256', 'dropout': 'True', 'epochs': '20.0', 'shape': 87, \n",
    "                   'preprocessing': 'filter', 'sequences': 5}}\n",
    "metricas = eval_variance(x_train, y_train, x_test, y_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16934df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Media:\", metricas['lstm']['mean'])\n",
    "print(\"Desvio:\", metricas['lstm']['stand_dev'])\n",
    "print(\"Intervalo:\", metricas['lstm']['conf_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "media, dp, ci = eval_one_variance(test, model, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9df73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f42df624",
   "metadata": {},
   "source": [
    "def delete_lstm():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    runs = mlflow.search_runs()\n",
    "\n",
    "    for run in runs.iterrows():\n",
    "        if run[1][\"params.model\"] == \"lstm\":\n",
    "            mlflow.delete_run(run[1].run_id)\n",
    "    print(\"Removidas lstms\")\n",
    "delete_lstm()    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5c6b29d",
   "metadata": {},
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de110249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data\")\n",
    "mat = get_data()\n",
    "print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "train, test = split(0.75, mat)\n",
    "print(\"Spliting the data into x and y features\")\n",
    "x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "split_strategy = \"train_test\"\n",
    "\n",
    "# params = get_best_parameters(split_strategy)\n",
    "params = {'lstm': {'learning_rate': 0.00061, 'units': '864.0', 'activation': 'tanh', \n",
    "                   'batch': '2048', 'dropout': 'True', 'epochs': '30.0', 'shape': 87, \n",
    "                   'preprocessing': 'fi_ss', 'sequences': 5}}\n",
    "\n",
    "eval_variance(x_train, y_train, x_test, y_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data\")\n",
    "mat = get_data()\n",
    "print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "train, test = split(0.75, mat)\n",
    "print(\"Spliting the data into x and y features\")\n",
    "x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "\n",
    "\n",
    "# print(\"Find best parameters for LSTM model\")\n",
    "# find_best(x_train, y_train, 10, search_space_lstm)\n",
    "\n",
    "x_train, y_train, x_test = preprocessing('filter', x_train, y_train, x_test)\n",
    "print(\"Converting training data\")\n",
    "y_train = ajusta_y(y_train)\n",
    "x_train, y_train = transform_dimension_timesteps(x_train, y_train, time_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6febff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "# Definir a semente para a geração de números aleatórios do numpy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definir a semente para a geração de números aleatórios do Python\n",
    "python_random.seed(42)\n",
    "\n",
    "# Definir a semente para a geração de números aleatórios do TensorFlow\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef95c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {'activation': 'relu',\n",
    "        'units': 192,\n",
    "        'batch': 2516,\n",
    "        'dropout': True,\n",
    "        'learning_rate': 0.00015209924599838263,\n",
    "        'shape': x_train.shape[2]}\n",
    "\n",
    "model = MyModel().build(**params)\n",
    "\n",
    "print(\"Training the model\")\n",
    "model.fit(x_train, y_train, batch_size=4096, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_keras(model, x_test)\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_p = pd.DataFrame(x_test)\n",
    "test = pd.concat([x_test_p.reset_index(drop=True),\n",
    "                  y_test.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_param(\"model\", 'lstm')\n",
    "        mlflow.log_param(\"stage\", \"statistics_analysis\")\n",
    "        mlflow.log_param(\"model_selection\", 'train_test')\n",
    "        mlflow.log_param(\"random_i\", i)\n",
    "\n",
    "        test_shuffle = test.sample(frac=.5, random_state=i)\n",
    "\n",
    "        x_test_n = test_shuffle.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "        y_test_n = test_shuffle[[\"INDISPONIBILIDADE\"]]\n",
    "\n",
    "        pred = predict_keras(model, x_test_n)\n",
    "\n",
    "        f1, roc, rec, pre, acc = eval_metrics(y_test_n, pred)\n",
    "\n",
    "        results.append(f1)\n",
    "\n",
    "        mlflow.log_metric('f1', f1)\n",
    "        mlflow.log_metric('roc', roc)\n",
    "        mlflow.log_metric('recall', rec)\n",
    "        mlflow.log_metric('precision', pre)\n",
    "        mlflow.log_metric('accuracy', acc)\n",
    "media = np.mean(results)\n",
    "dp = np.std(results, ddof=1)\n",
    "ci = sms.DescrStatsW(results).tconfint_mean()\n",
    "print(media, dp, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5329b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
