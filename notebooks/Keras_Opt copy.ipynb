{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562aabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, roc_auc_score, \\\n",
    "    recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "import json\n",
    "import mlflow\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.stats.api as sms\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e0ac790",
   "metadata": {},
   "source": [
    "search_space_lstm = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'lstm',\n",
    "        'activation': hp.choice('activation', [\"relu\", \"tanh\"]),\n",
    "        'units': hp.quniform('units', 32, 1024, 32),\n",
    "        'batch': hp.choice('batch', [2048]),\n",
    "        'epochs': hp.quniform('epochs', 10, 50, 10),\n",
    "        'dropout': hp.choice('dropout', [True, False]),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.01)),\n",
    "        'preprocessing': hp.choice('p_lstm', ['scaler', 'filter', 'all', 'none', 'fi_ss',\n",
    "                                    'fi_sm', 'ss_sm', 'smote'])\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b467d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_lstm = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'lstm',\n",
    "        'activation': hp.choice('activation', [\"relu\"]),\n",
    "        'units': hp.quniform('units', 576, 960, 32),\n",
    "        'batch': hp.choice('batch', [2048]),\n",
    "        'epochs': hp.quniform('epochs', 30, 70, 10),\n",
    "        'dropout': hp.choice('dropout', [True, False]),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.006)),\n",
    "        'preprocessing': hp.choice('p_lstm', ['scaler', 'filter', 'all', 'none', 'fi_ss',\n",
    "                                    'fi_sm', 'ss_sm', 'smote'])\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf99125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_0(y):\n",
    "    if y['INDISPONIBILIDADE'] == 0:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "def f_1(y):\n",
    "    if y['INDISPONIBILIDADE'] == 1:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "def ajusta_y(y):\n",
    "    y['0'] = y.apply(f_0, axis=1)\n",
    "    y['1'] = y.apply(f_1, axis=1)\n",
    "    y = y[['0', '1']]\n",
    "    return y\n",
    "\n",
    "\n",
    "#def create_sequences(values, time_steps=1):\n",
    "#    output = []\n",
    "#    for i in range(len(values) - time_steps + 1):\n",
    "#        output.append(values[i:(i + time_steps)])\n",
    "#    return np.stack(output)\n",
    "\n",
    "\n",
    "def create_sequences(values, time_steps=1):\n",
    "    return np.asarray([values[i : (i + time_steps)] for i in range(len(values) - time_steps + 1)])\n",
    "\n",
    "\n",
    "def ajusta_y_timestep(y, time_steps=1):\n",
    "    new_y = y[time_steps-1:]\n",
    "    return new_y\n",
    "\n",
    "\n",
    "def transform_dimension_timesteps(train_x, train_y, time_steps=1):\n",
    "\n",
    "    train_x = create_sequences(train_x, time_steps)\n",
    "    train_y = ajusta_y_timestep(train_y, time_steps)\n",
    "    train_y = train_y.values.reshape(-1, 2)\n",
    "    \n",
    "    print(train_y.shape)\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def ajusta_saida(y_pred):\n",
    "    y_pred_c = []\n",
    "    for x in y_pred:\n",
    "        y_pred_c.append(np.argmax(x))\n",
    "    return y_pred_c\n",
    "\n",
    "\"\"\"def objective_lstm(params):\n",
    "    units = params['units']\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(params['units'], activation=params['activation'],\n",
    "                   return_sequences=False, input_shape=(1, shape)))\n",
    "    \n",
    "    model.add(Dense(params['units'], activation=params['activation'],\n",
    "                    input_shape=(784,)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=params['epochs'],\n",
    "              batch_size=params['batch_size'], verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return -score[1]  \n",
    "\"\"\"\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "class MyModel():\n",
    "    def build(self, **kwargs):\n",
    "        activation = kwargs.get('activation')\n",
    "        shape = kwargs.get('shape')\n",
    "        batch = int(kwargs.get('batch'))\n",
    "        dropout = kwargs.get('dropout')\n",
    "        lr = kwargs.get('learning_rate')\n",
    "        units = int(float(kwargs.get('units')))\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(LSTM(units, activation=activation, return_sequences=False,\n",
    "                       input_shape=(1, shape)))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        if dropout:\n",
    "            model.add(Dropout(rate=0.2))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=RMSprop(learning_rate=lr),\n",
    "                      metrics=[get_f1])\n",
    "        return model\n",
    "\n",
    "    def fit(self, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=batch,\n",
    "            **kwargs, verbose=0\n",
    "        )\n",
    "\n",
    "\n",
    "def predict_keras(model, test_x):\n",
    "    x = create_sequences(test_x.copy(), 1)\n",
    "    predicted = model.predict(x)\n",
    "    predicted = ajusta_saida(predicted)\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def test_model(model, l, x_train, y_train, x_val, y_val):\n",
    "    batch, epochs = l\n",
    "    # print(\"Converting training data\")\n",
    "    # x, y = transform_dimension_timesteps(train_x.copy(), train_y)\n",
    "\n",
    "    print(\"Training the model\")\n",
    "    model.fit(x_train, y_train, batch_size=batch, epochs=epochs)\n",
    "\n",
    "    pred = predict_keras(model, x_val)\n",
    "    f1 = f1_score(y_val, pred)\n",
    "    print(confusion_matrix(y_val, pred))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def split(r, mat):\n",
    "    i = int(len(mat)*r)\n",
    "\n",
    "    return mat[:i], mat[i:]\n",
    "\n",
    "\n",
    "def train_test(train, test):\n",
    "\n",
    "    x_train = train.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_train = train[['INDISPONIBILIDADE']]\n",
    "\n",
    "    x_test = test.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_test = test[['INDISPONIBILIDADE']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def find_best_keras(df, evals):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2156b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    f1 = f1_score(actual, pred)\n",
    "    roc = roc_auc_score(actual, pred)\n",
    "    rec = recall_score(actual, pred)\n",
    "    pre = precision_score(actual, pred)\n",
    "    acc = accuracy_score(actual, pred)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    print(confusion_matrix(actual, pred))\n",
    "    return f1, roc, rec, pre, acc\n",
    "\n",
    "\n",
    "def model_selection(m, x, y, p, c, clf):\n",
    "    if m == 'train_test':\n",
    "        # Ratio train test split\n",
    "        r = 0.75\n",
    "        return train_test_selection(r, x, y, p, c, clf)\n",
    "\n",
    "\n",
    "\n",
    "def objective_keras(params):\n",
    "    \n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        x, y = data\n",
    "        p = params['preprocessing']\n",
    "        print(\"Preprocess:\", p)\n",
    "        df = pd.concat([x.reset_index(drop=True),\n",
    "                    y.reset_index(drop=True)], axis=1) \n",
    "        train, test = split(.75, df)\n",
    "        x_train, y_train, x_val, y_val = train_test(train, test)\n",
    "\n",
    "        x_train, x_val, y_train = preprocessing(p, x_train, x_val, y_train)\n",
    "        print(\"Converting training data\")\n",
    "        y_train = ajusta_y(y_train)\n",
    "        x_train, y_train = transform_dimension_timesteps(x_train, y_train, time_steps=1)\n",
    "        \n",
    "        \n",
    "        del params['preprocessing']\n",
    "        del params['type']\n",
    "        mlflow.log_param(\"model\", 'lstm')\n",
    "        mlflow.log_param(\"model_selection\", split_strategy)\n",
    "        mlflow.log_param(\"stage\", 'tuning')\n",
    "\n",
    "        params['shape'] = x_train.shape[2]\n",
    "        clf = MyModel().build(**params)\n",
    "        l = [int(params['batch']), int(params['epochs'])]\n",
    "\n",
    "        print(params)\n",
    "\n",
    "        f1 = test_model(clf, l, x_train, y_train, x_val, y_val)\n",
    "        \n",
    "        f1 = f1 + test_model(clf, l, x_train, y_train, x_val, y_val)\n",
    "        \n",
    "        f1 = f1 + test_model(clf, l, x_train, y_train, x_val, y_val)\n",
    "        \n",
    "        f1 = f1/3\n",
    "\n",
    "        print(\"Média F1-SCORE\", f1)\n",
    "        mlflow.log_metric(\"f1_val\", f1)\n",
    "\n",
    "        # Because fmin() tries to minimize the objective,\n",
    "        # this function must return the negative accuracy.\n",
    "        return {'loss': -f1, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def get_best(key):\n",
    "    f = open('params/best_hyper.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data[key]\n",
    "\n",
    "\n",
    "def find_best(x, y, evals, space):\n",
    "    \n",
    "\n",
    "    global data\n",
    "    data = [x, y]\n",
    "    rstate = np.random.default_rng(42)\n",
    "    trials = Trials()\n",
    "    best_result = fmin(\n",
    "        fn=objective_keras, space=space,\n",
    "        algo=tpe.suggest, max_evals=evals,\n",
    "        trials=trials, rstate=rstate)\n",
    "\n",
    "    result = hyperopt.space_eval(space, best_result)\n",
    "    print(\"Best in Search Space:\", result)\n",
    "    print('trials:')\n",
    "    for trial in trials.trials[:2]:\n",
    "        print(trial)\n",
    "\n",
    "    key = result['type']\n",
    "    del result['type']\n",
    "    # update_hyper(result, key)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    return result, trials, key\n",
    "\n",
    "\n",
    "def split(r, mat):\n",
    "    i = int(len(mat)*r)\n",
    "\n",
    "    return mat[:i], mat[i:]\n",
    "\n",
    "\n",
    "def preprocess(filtering, scaler, smote, x_train, x_test, y_train):\n",
    "\n",
    "    if filtering == 'True':\n",
    "        print(\"Filtering\")\n",
    "        with open('../data/params/features.pkl', 'rb') as inp:\n",
    "            features = pickle.load(inp)\n",
    "        x_train = x_train[features]\n",
    "        x_test = x_test[features]\n",
    "\n",
    "    if scaler == 'True':\n",
    "        print(\"Standard Scale\")\n",
    "        ss = StandardScaler() # .set_output(transform=\"pandas\")\n",
    "        ss.fit(x_train)\n",
    "        x_train = ss.transform(x_train)\n",
    "        x_test = ss.transform(x_test)\n",
    "\n",
    "    if smote == 'True':\n",
    "        print(\"SMOTE\")\n",
    "\n",
    "        # if isinstance(x_train, cd.DataFrame):\n",
    "        #    x_train, y_train = x_train.to_pandas(), y_train.to_pandas()\n",
    "        \n",
    "        with open(\"../data/params/smote.pkl\", \"rb\") as inp:\n",
    "            samp_strat = pickle.load(inp)\n",
    "        print(\"Sampling Strategy: \", samp_strat)\n",
    "        smote = SMOTE(random_state=42, sampling_strategy=samp_strat)\n",
    "        x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "        # x_train, y_train = cd.from_pandas(x_train), cd.from_pandas(y_train)\n",
    "\n",
    "    return x_train, x_test, y_train\n",
    "\n",
    "\n",
    "def preprocessing(p, x_train, x_test,  y_train, ):\n",
    "\n",
    "    if p == 'all':\n",
    "        x_train, x_test, y_train = preprocess('True', 'True', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'filter':\n",
    "        x_train, x_test, y_train = preprocess('True', 'False', 'False',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'scaler':\n",
    "        x_train, x_test, y_train = preprocess('False', 'True', 'False',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'smote':\n",
    "        x_train, x_test, y_train = preprocess('False', 'False', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'fi_sm':\n",
    "        x_train, x_test, y_train = preprocess('True', 'False', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'fi_ss':\n",
    "        x_train, x_test, y_train = preprocess('True', 'True', 'False',\n",
    "                                              x_train, x_test, y_train)\n",
    "    elif p == 'ss_sm':\n",
    "        x_train, x_test, y_train = preprocess('False', 'True', 'True',\n",
    "                                              x_train, x_test, y_train)\n",
    "\n",
    "    return x_train, x_test, y_train\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    mat = pd.read_csv('../data/raw/matomo.csv', dtype=np.int32)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def train_test(train, test):\n",
    "\n",
    "    x_train = train.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_train = train[['INDISPONIBILIDADE']]\n",
    "\n",
    "    x_test = test.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "    y_test = test[['INDISPONIBILIDADE']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def get_model(k, params):\n",
    "    if k == 'knn':\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "    elif k == 'svm':\n",
    "        clf = SVC(**params)\n",
    "    elif k == 'nb':\n",
    "        clf = GaussianNB(**params)\n",
    "    elif k == 'rf':\n",
    "        clf = RandomForestClassifier(**params)\n",
    "    elif k == 'ada':\n",
    "        clf = AdaBoostClassifier(**params)\n",
    "    elif k == 'dt':\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "    elif k == 'lstm':\n",
    "        clf = MyModel().build(**params)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def test_one(key, params, x_train, y_train, x_test, y_test):\n",
    "    p = params['preprocessing']\n",
    "    del params['preprocessing']\n",
    "\n",
    "    x_train, x_test, y_train = preprocessing(p, x_train,\n",
    "                                             x_test, y_train)\n",
    "\n",
    "    model = get_model(key, params)\n",
    "\n",
    "    if key in ('ada', 'dt', 'nb'):\n",
    "        model.fit(x_train.to_pandas(), y_train.to_pandas())\n",
    "        pred = model.predict(x_test.to_pandas())\n",
    "        f1, roc, rec, pre, acc = eval_metrics(y_test.to_pandas(), pred)\n",
    "    else:\n",
    "        y_train = y_train['INDISPONIBILIDADE'].values\n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        f1, roc, rec, pre, acc = eval_metrics(y_test.to_pandas(),\n",
    "                                              pred.to_pandas())\n",
    "\n",
    "    return f1, roc, rec, pre, acc\n",
    "\n",
    "\n",
    "def test_params(x_train, y_train, x_test, y_test, params):\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "    for key in params:\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_param(\"model\", key)\n",
    "            mlflow.log_param(\"model_selection\", split_strategy)\n",
    "            mlflow.log_param(\"stage\", \"Testing_algos\")\n",
    "            mlflow.log_params(params[key])\n",
    "\n",
    "            print(key)\n",
    "            f1, roc, rec, pre, acc = test_one(key, params[key],\n",
    "                                              x_train, y_train,\n",
    "                                              x_test, y_test)\n",
    "\n",
    "            mlflow.log_metric('f1', f1)\n",
    "            mlflow.log_metric('roc', roc)\n",
    "            mlflow.log_metric('recall', rec)\n",
    "            mlflow.log_metric('precision', pre)\n",
    "            mlflow.log_metric('accuracy', acc)\n",
    "\n",
    "\n",
    "def delete_runs():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    runs = mlflow.search_runs()\n",
    "\n",
    "    for run in runs.iterrows():\n",
    "        mlflow.delete_run(run[1].run_id)\n",
    "\n",
    "\n",
    "def is_sklearn(model):\n",
    "    if type(model).__module__[:7] == 'sklearn':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def ajust_columns(results):\n",
    "    for c in results.columns:\n",
    "        if c[:7] == \"params.\":\n",
    "            results = results.rename(columns={c: c[7:]})\n",
    "\n",
    "    results = results.rename(columns={\"metrics.f1_val\": \"f1_val\"})\n",
    "    return results\n",
    "\n",
    "\n",
    "def correct_parameters(best_results):\n",
    "    for result in best_results:\n",
    "        if result != \"knn\":\n",
    "            try:\n",
    "                del best_results[result][\"n_neighbors\"]\n",
    "                del best_results[result][\"metric\"]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    for k in best_results:\n",
    "        for p in best_results[k]:\n",
    "            if p in (\"n_estimators\", \"n_neighbors\"):\n",
    "                best_results[k][p] = int(best_results[k][p])\n",
    "            if p in (\"C\", \"var_smoothing\", \"learning_rate\"):\n",
    "                best_results[k][p] = float(best_results[k][p])\n",
    "\n",
    "    return best_results\n",
    "\n",
    "\n",
    "def get_best_parameters(split_strategy):\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    results = mlflow.search_runs()\n",
    "\n",
    "    results = ajust_columns(results)\n",
    "    query = f'model_selection == \"{split_strategy}\"'\n",
    "    grouped = results.query(query).groupby(\"type\")\n",
    "    indices_max = grouped[\"f1_val\"].idxmax()\n",
    "    best_results = {}\n",
    "\n",
    "    for modelo, indice in indices_max.items():\n",
    "        parametros = results.loc[\n",
    "            indice,\n",
    "            [\n",
    "                \"preprocessing\",\n",
    "                \"C\",\n",
    "                \"kernel\",\n",
    "                \"n_estimators\",\n",
    "                \"n_neighbors\",\n",
    "                \"criterion\",\n",
    "                \"var_smoothing\",\n",
    "                \"learning_rate\",\n",
    "                \"metric\",\n",
    "                \"units\",\n",
    "                \"activation\",\n",
    "                \"batch\",\n",
    "                \"dropout\", \n",
    "                \"epochs\"\n",
    "            ],\n",
    "        ]\n",
    "        parametros = {\n",
    "            chave: valor\n",
    "            for chave, valor in parametros.to_dict().items()\n",
    "            if type(valor) == str\n",
    "        }\n",
    "        best_results[modelo] = parametros\n",
    "\n",
    "    return correct_parameters(best_results)\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "    print(\"Reading data\")\n",
    "    mat = get_data()\n",
    "    print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "    train, test = split(0.75, mat)\n",
    "    print(\"Spliting the data into x and y features\")\n",
    "    x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "\n",
    "   \n",
    "    print(\"Find best parameters for LSTM model\")\n",
    "    find_best(x_train, y_train, 50, search_space_lstm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c0c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_variance(test, model, key):\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_param(\"model\", key)\n",
    "            mlflow.log_param(\"stage\", \"statistics_analysis\")\n",
    "            # mlflow.log_param(\"model_selection\", split_strategy)\n",
    "            mlflow.log_param(\"random_i\", i)\n",
    "\n",
    "            test_shuffle = test.sample(frac=0.5, random_state=i)\n",
    "\n",
    "            x_test = test_shuffle.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "            y_test = test_shuffle[[\"INDISPONIBILIDADE\"]]\n",
    "            \n",
    "            # y_test = ajusta_y(y_test)\n",
    "            print(\"Transforming dimension\")\n",
    "            # x_test, y_test = transform_dimension_timesteps(x_test, y_test, time_steps=1)\n",
    "\n",
    "            pred = predict_keras(model, x_test) # model.predict(x_test)\n",
    "            pred = ajusta_saida(pred)\n",
    "            \n",
    "            # pred = pred.to_numpy()\n",
    "            y_test = y_test.values\n",
    "\n",
    "            f1, roc, rec, pre, acc = eval_metrics(y_test, pred)\n",
    "            results.append(f1)\n",
    "\n",
    "            mlflow.log_metric(\"f1\", f1)\n",
    "            mlflow.log_metric(\"roc\", roc)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"precision\", pre)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    media = np.mean(results)\n",
    "    dp = np.std(results, ddof=1)\n",
    "    ci = sms.DescrStatsW(results).tconfint_mean()\n",
    "    return media, dp, ci\n",
    "\n",
    "\n",
    "def eval_variance(x_train, y_train, x_test, y_test, params):\n",
    "    metricas = {}\n",
    "    for k in params:\n",
    "        if k in [\"lstm\"]:\n",
    "            print(\"Algo:\", k)\n",
    "            p = params[k][\"preprocessing\"]\n",
    "            del params[k][\"preprocessing\"]\n",
    "            (\n",
    "                x_train_c,\n",
    "                x_test_c,\n",
    "                y_train_c,\n",
    "            ) = preprocessing(p, x_train, x_test, y_train)\n",
    "            params[k][\"shape\"] = x_train_c.shape[1]\n",
    "            print(params[k])\n",
    "            model = MyModel().build(**params[k])\n",
    "\n",
    "            print(\"Ajusting y\")\n",
    "            y_train_c = ajusta_y(y_train_c)\n",
    "            print(\"Transforming dimension\")\n",
    "            x_train_c, y_train_c = transform_dimension_timesteps(x_train_c, y_train_c, time_steps=1)\n",
    "            # y_train_c = y_train_c[\"INDISPONIBILIDADE\"].values\n",
    "            batch = int(params[k][\"batch\"])\n",
    "            print(\"Fitting model\")\n",
    "            model.fit(x_train_c, y_train_c, batch_size=batch, epochs=int(float(params[k][\"epochs\"])))\n",
    "            \n",
    "            (\n",
    "                x_train_c,\n",
    "                x_test_c,\n",
    "                y_train_c,\n",
    "            ) = preprocessing(p, x_train, x_test, y_train)\n",
    "            \n",
    "            \n",
    "            x_test_c = pd.DataFrame(x_test_c)\n",
    "            test = pd.concat(\n",
    "                [x_test_c.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1\n",
    "            )\n",
    "\n",
    "            media, dp, ci = eval_one_variance(test, model, k)\n",
    "            metricas[k] = {}\n",
    "            metricas[k][\"mean\"] = media\n",
    "            metricas[k][\"stand_dev\"] = dp\n",
    "            metricas[k][\"conf_int\"] = ci\n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d401305",
   "metadata": {},
   "source": [
    "def delete_lstm():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    runs = mlflow.search_runs()\n",
    "\n",
    "    for run in runs.iterrows():\n",
    "        if run[1][\"params.model\"] == \"lstm\":\n",
    "            mlflow.delete_run(run[1].run_id)\n",
    "    print(\"Removidas lstms\")\n",
    "delete_lstm()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Spliting the data into train/test with 75/25 proportion\n",
      "Spliting the data into x and y features\n",
      "Find best parameters for LSTM model\n",
      "Preprocess:                                                                                                            \n",
      "fi_sm                                                                                                                  \n",
      "Filtering                                                                                                              \n",
      "SMOTE                                                                                                                  \n",
      "Sampling Strategy:                                                                                                     \n",
      "0.004664936141760721                                                                                                   \n",
      "Converting training data                                                                                               \n",
      "  0%|                                                                           | 0/50 [00:02<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "global split_strategy\n",
    "split_strategy = 'train_test'\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de110249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Spliting the data into train/test with 75/25 proportion\n",
      "Spliting the data into x and y features\n",
      "{'ada': {'preprocessing': 'ss_sm', 'n_estimators': 50, 'learning_rate': 1.0}, 'dt': {'preprocessing': 'fi_sm', 'criterion': 'entropy'}, 'knn': {'preprocessing': 'all', 'n_neighbors': 1, 'metric': 'manhattan'}, 'lstm': {'preprocessing': 'fi_ss', 'learning_rate': 0.0006299586720690262, 'units': '768.0', 'activation': 'tanh', 'batch': '2048', 'dropout': 'True', 'epochs': '50.0'}, 'nb': {'preprocessing': 'none', 'var_smoothing': 1e-05}, 'rf': {'preprocessing': 'none', 'n_estimators': 100}, 'svm': {'preprocessing': 'filter', 'C': 1.0, 'kernel': 'rbf'}}\n",
      "Algo: lstm\n",
      "Filtering\n",
      "Standard Scale\n",
      "{'learning_rate': 0.0006299586720690262, 'units': '768.0', 'activation': 'tanh', 'batch': '2048', 'dropout': 'True', 'epochs': '50.0', 'shape': 87}\n",
      "Ajusting y\n",
      "Transforming dimension\n",
      "(600000, 2)\n",
      "Fitting model\n",
      "Epoch 1/50\n",
      "293/293 [==============================] - 169s 8ms/step - loss: 0.0202 - get_f1: 0.9928\n",
      "Epoch 2/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0041 - get_f1: 0.9991\n",
      "Epoch 3/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0036 - get_f1: 0.9990\n",
      "Epoch 4/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0030 - get_f1: 0.9993\n",
      "Epoch 5/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0027 - get_f1: 0.9994\n",
      "Epoch 6/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0021 - get_f1: 0.9996\n",
      "Epoch 7/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0023 - get_f1: 0.9996\n",
      "Epoch 8/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0020 - get_f1: 0.9997\n",
      "Epoch 9/50\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.0019 - get_f1: 0.9997\n",
      "Epoch 10/50\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.0028 - get_f1: 0.9993\n",
      "Epoch 11/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0022 - get_f1: 0.9996\n",
      "Epoch 12/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0016 - get_f1: 0.9997\n",
      "Epoch 13/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0015 - get_f1: 0.9998\n",
      "Epoch 14/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0015 - get_f1: 0.9997\n",
      "Epoch 15/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0015 - get_f1: 0.9998\n",
      "Epoch 16/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0014 - get_f1: 0.9998\n",
      "Epoch 17/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0019 - get_f1: 0.9994\n",
      "Epoch 18/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0013 - get_f1: 0.9998\n",
      "Epoch 19/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0013 - get_f1: 0.9998\n",
      "Epoch 20/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0014 - get_f1: 0.9998\n",
      "Epoch 21/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0012 - get_f1: 0.9998\n",
      "Epoch 22/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0022 - get_f1: 0.9994\n",
      "Epoch 23/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0012 - get_f1: 0.9998\n",
      "Epoch 24/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0013 - get_f1: 0.9998\n",
      "Epoch 25/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0016 - get_f1: 0.9997\n",
      "Epoch 26/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0012 - get_f1: 0.9998\n",
      "Epoch 27/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0012 - get_f1: 0.9998\n",
      "Epoch 28/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0012 - get_f1: 0.9998\n",
      "Epoch 29/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0011 - get_f1: 0.9998\n",
      "Epoch 30/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0011 - get_f1: 0.9998\n",
      "Epoch 31/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0022 - get_f1: 0.9995\n",
      "Epoch 32/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0011 - get_f1: 0.9998\n",
      "Epoch 33/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0016 - get_f1: 0.9996\n",
      "Epoch 34/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0018 - get_f1: 0.9996\n",
      "Epoch 35/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0011 - get_f1: 0.9998\n",
      "Epoch 36/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 37/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 38/50\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 39/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0011 - get_f1: 0.9998\n",
      "Epoch 40/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 41/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 42/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0011 - get_f1: 0.9999\n",
      "Epoch 43/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0011 - get_f1: 0.9998\n",
      "Epoch 44/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 9.8696e-04 - get_f1: 0.9998\n",
      "Epoch 45/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 9.6464e-04 - get_f1: 0.9998\n",
      "Epoch 46/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.0010 - get_f1: 0.9998\n",
      "Epoch 47/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 9.8004e-04 - get_f1: 0.9999\n",
      "Epoch 48/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 9.2197e-04 - get_f1: 0.9998\n",
      "Epoch 49/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 9.4578e-04 - get_f1: 0.9998\n",
      "Epoch 50/50\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 9.8740e-04 - get_f1: 0.9998\n",
      "Filtering\n",
      "Standard Scale\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99546     0]\n",
      " [  454     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99561     0]\n",
      " [  439     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99532     0]\n",
      " [  468     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99521     0]\n",
      " [  479     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99529     0]\n",
      " [  471     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99557     0]\n",
      " [  443     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99550     0]\n",
      " [  450     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99574     0]\n",
      " [  426     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99531     0]\n",
      " [  469     0]]\n",
      "Transforming dimension\n",
      "F1-Score: 0.0\n",
      "[[99514     0]\n",
      " [  486     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lstm': {'mean': 0.0, 'stand_dev': 0.0, 'conf_int': (0.0, 0.0)}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "mat = get_data()\n",
    "print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "train, test = split(0.75, mat)\n",
    "print(\"Spliting the data into x and y features\")\n",
    "x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "split_strategy = \"train_test\"\n",
    "params = get_best_parameters(split_strategy)\n",
    "print(params)\n",
    "eval_variance(x_train, y_train, x_test, y_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data\")\n",
    "mat = get_data()\n",
    "print(\"Spliting the data into train/test with 75/25 proportion\")\n",
    "train, test = split(0.75, mat)\n",
    "print(\"Spliting the data into x and y features\")\n",
    "x_train, y_train, x_test, y_test = train_test(train, test)\n",
    "\n",
    "\n",
    "# print(\"Find best parameters for LSTM model\")\n",
    "# find_best(x_train, y_train, 10, search_space_lstm)\n",
    "\n",
    "x_train, y_train, x_test = preprocessing('filter', x_train, y_train, x_test)\n",
    "print(\"Converting training data\")\n",
    "y_train = ajusta_y(y_train)\n",
    "x_train, y_train = transform_dimension_timesteps(x_train, y_train, time_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6febff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "# Definir a semente para a geração de números aleatórios do numpy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definir a semente para a geração de números aleatórios do Python\n",
    "python_random.seed(42)\n",
    "\n",
    "# Definir a semente para a geração de números aleatórios do TensorFlow\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef95c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {'activation': 'relu',\n",
    "        'units': 192,\n",
    "        'batch': 2516,\n",
    "        'dropout': True,\n",
    "        'learning_rate': 0.00015209924599838263,\n",
    "        'shape': x_train.shape[2]}\n",
    "\n",
    "model = MyModel().build(**params)\n",
    "\n",
    "print(\"Training the model\")\n",
    "model.fit(x_train, y_train, batch_size=4096, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_keras(model, x_test)\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_p = pd.DataFrame(x_test)\n",
    "test = pd.concat([x_test_p.reset_index(drop=True),\n",
    "                  y_test.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_param(\"model\", 'lstm')\n",
    "        mlflow.log_param(\"stage\", \"statistics_analysis\")\n",
    "        mlflow.log_param(\"model_selection\", 'train_test')\n",
    "        mlflow.log_param(\"random_i\", i)\n",
    "\n",
    "        test_shuffle = test.sample(frac=.5, random_state=i)\n",
    "\n",
    "        x_test_n = test_shuffle.drop([\"INDISPONIBILIDADE\"], axis=1)\n",
    "        y_test_n = test_shuffle[[\"INDISPONIBILIDADE\"]]\n",
    "\n",
    "        pred = predict_keras(model, x_test_n)\n",
    "\n",
    "        f1, roc, rec, pre, acc = eval_metrics(y_test_n, pred)\n",
    "\n",
    "        results.append(f1)\n",
    "\n",
    "        mlflow.log_metric('f1', f1)\n",
    "        mlflow.log_metric('roc', roc)\n",
    "        mlflow.log_metric('recall', rec)\n",
    "        mlflow.log_metric('precision', pre)\n",
    "        mlflow.log_metric('accuracy', acc)\n",
    "media = np.mean(results)\n",
    "dp = np.std(results, ddof=1)\n",
    "ci = sms.DescrStatsW(results).tconfint_mean()\n",
    "print(media, dp, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5329b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
